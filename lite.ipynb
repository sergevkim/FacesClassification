{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "\n",
    "import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size : \t1\n",
      "checkpoints_dir : \t/home/sergevkim/git/FacesClassification/checkpoints\n",
      "checkpoint_filename : \t\n",
      "disable_cuda : \tFalse\n",
      "imgs_dir : \t/home/sergevkim/git/FacesClassification/data/CelebaHQ\n",
      "label : \tMale\n",
      "labels_filename : \t/home/sergevkim/git/FacesClassification/data/list_attr_celeba.txt\n",
      "logs_dir : \t/home/sergevkim/git/FacesClassification/logs\n",
      "n_epochs : \t10\n",
      "n_imgs : \t30000\n",
      "verbose : \tTrue\n",
      "version : \t0.1\n",
      "device : \tcuda:0\n"
     ]
    }
   ],
   "source": [
    "from lib.constants import HYPERPARAMETERS\n",
    "\n",
    "params = HYPERPARAMETERS\n",
    "params['batch_size'] = 1\n",
    "params['n_epochs'] = 10\n",
    "params['verbose'] = True\n",
    "params['version'] = '0.1'\n",
    "\n",
    "if not params['disable_cuda'] and torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "params['device'] = device\n",
    "\n",
    "for p in params:\n",
    "    print(f\"{p} : \\t{params[p]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_phase(model, train_loader, criterion, optimizer, device, epoch, verbose=False):\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs).double() #outputs is from 0 to 1\n",
    "        outputs_2 = outputs * 2 - 1 #is it differentiate?\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose:\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('train', batch_idx, loss.item())\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    print(f\"{epoch}: done!\")\n",
    "    return loss_history\n",
    "\n",
    "\n",
    "def valid_phase(model, valid_loader, device, epoch, verbose=False):\n",
    "    model.eval()\n",
    "    accuracy_scores_history = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(valid_loader):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs).double() #outputs is from 0 to 1\n",
    "        outputs = outputs.cpu().detach().numpy()\n",
    "        # do have inputs be cuda-like if model is cuda-like?\n",
    "        predicts = outputs.round() * 2 - 1\n",
    "        #predicts.cpu()?\n",
    "\n",
    "        batch_accuracy = accuracy_score(predicts, labels)\n",
    "\n",
    "        accuracy_scores_history.append(batch_accuracy)\n",
    "\n",
    "    return accuracy_scores_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergevkim/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:516: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0 0.1538991152061585\n",
      "train 100 -1.5416062668453696\n",
      "train 200 -3.599171088190458\n",
      "train 300 9.273326333770317\n",
      "train 400 16.470804322702566\n",
      "train 500 -27.527740411353864\n",
      "train 600 -18.44717025610011\n",
      "train 700 -30.59137345770687\n",
      "train 800 -42.38199229607343\n",
      "train 900 -36.760303496061695\n",
      "train 1000 -44.87329105272902\n",
      "train 1100 -30.90599439365328\n",
      "train 1200 -31.987195987486245\n",
      "train 1300 -34.42033388739677\n",
      "train 1400 34.38559717030953\n",
      "train 1500 -46.56291573555032\n",
      "train 1600 39.53678500275602\n",
      "train 1700 41.623565725812924\n",
      "train 1800 -29.31692509509219\n",
      "train 1900 48.765808196672445\n",
      "train 2000 60.68291473922667\n",
      "train 2100 53.133899750336624\n",
      "train 2200 -55.00442505564123\n",
      "train 2300 -42.127765653939285\n",
      "train 2400 -42.938808381827194\n",
      "train 2500 -49.586856896695366\n",
      "train 2600 -53.99350730758507\n",
      "train 2700 56.30299375414248\n",
      "train 2800 -55.62794497129882\n",
      "train 2900 -58.97510150764208\n",
      "train 3000 66.9883956904567\n",
      "train 3100 33.30948254172417\n",
      "train 3200 -31.79667853270235\n",
      "train 3300 -62.810653672613284\n",
      "train 3400 -51.198169793557284\n",
      "train 3500 54.61052313962774\n",
      "train 3600 51.58182527887622\n",
      "train 3700 73.68006897818304\n",
      "train 3800 58.33201601388422\n",
      "train 3900 -36.17485039339601\n",
      "train 4000 -66.55391686732695\n",
      "train 4100 57.48789593215018\n",
      "train 4200 -69.29938501729163\n",
      "train 4300 -55.86333847278365\n",
      "train 4400 -65.11352547568114\n",
      "train 4500 57.71374887598526\n",
      "train 4600 49.56571966653864\n",
      "train 4700 -88.50810253476479\n",
      "train 4800 -73.47511290160833\n",
      "train 4900 -50.74797822964098\n",
      "train 5000 85.02239227784642\n",
      "train 5100 -77.96707148774324\n",
      "train 5200 -100.0\n",
      "train 5300 71.07479101790675\n",
      "train 5400 100.0\n",
      "train 5500 -63.11416629443325\n",
      "train 5600 -59.40286258500539\n",
      "train 5700 65.04358677502613\n",
      "train 5800 -100.0\n",
      "train 5900 -67.27620694301315\n",
      "train 6000 83.55023197882724\n",
      "train 6100 84.55441287180616\n",
      "train 6200 -51.93961713356081\n",
      "train 6300 -56.94997024941361\n",
      "train 6400 -100.0\n",
      "train 6500 70.80790710073352\n",
      "train 6600 -39.246883398503364\n",
      "train 6700 49.81395718125838\n",
      "train 6800 -55.21816633392875\n",
      "train 6900 -42.10911185738329\n",
      "train 7000 65.90940097017622\n",
      "train 7100 -70.74820706127076\n",
      "train 7200 56.97358314920178\n",
      "train 7300 41.54044724439179\n",
      "train 7400 -40.359241430889654\n",
      "train 7500 -63.847793633177005\n",
      "train 7600 42.91783906290193\n",
      "train 7700 64.2719116528371\n",
      "train 7800 -64.82263195632396\n",
      "train 7900 64.13388065518848\n",
      "train 8000 35.589717845633295\n",
      "train 8100 -77.30038460911743\n",
      "train 8200 67.12448114161249\n",
      "train 8300 84.50884242106999\n",
      "train 8400 50.784286448083044\n",
      "train 8500 -49.06753926198953\n",
      "train 8600 -77.28181462097281\n",
      "train 8700 76.12406918764572\n",
      "train 8800 68.84858703843423\n",
      "train 8900 -78.93704980834866\n",
      "train 9000 -65.91216280149311\n",
      "train 9100 -66.50205232186535\n",
      "train 9200 -84.43666075969999\n",
      "train 9300 68.09561154672572\n",
      "train 9400 -88.24213407053723\n",
      "train 9500 -41.34127429818169\n",
      "train 9600 42.17041784590399\n",
      "train 9700 -43.91813270752995\n",
      "train 9800 79.4885329536809\n",
      "train 9900 -67.3404387913291\n",
      "train 10000 50.582252459453564\n",
      "train 10100 -85.30558775222994\n",
      "train 10200 71.73008730341455\n",
      "train 10300 55.23473353746047\n",
      "train 10400 83.65567015804567\n",
      "train 10500 100.0\n",
      "train 10600 82.49487311873722\n",
      "train 10700 -74.9195252570183\n",
      "train 10800 -61.805442790172606\n",
      "train 10900 -100.0\n",
      "train 11000 84.10512539762425\n",
      "train 11100 -100.0\n",
      "train 11200 -73.45813744811683\n",
      "train 11300 72.78430951738848\n",
      "train 11400 -100.0\n",
      "train 11500 100.0\n",
      "train 11600 78.67179871329772\n",
      "train 11700 78.6466751632938\n",
      "train 11800 100.0\n",
      "train 11900 63.279499050665805\n",
      "train 12000 100.0\n",
      "train 12100 -78.21321113909994\n",
      "train 12200 100.0\n",
      "train 12300 86.14168553365609\n",
      "train 12400 -100.0\n",
      "train 12500 -74.44968411052969\n",
      "train 12600 -59.59883124409157\n",
      "train 12700 -70.05387879774482\n",
      "train 12800 -100.0\n",
      "train 12900 -100.0\n",
      "train 13000 -100.0\n",
      "train 13100 100.0\n",
      "train 13200 100.0\n",
      "train 13300 -66.22245789535705\n",
      "train 13400 -100.0\n",
      "train 13500 -100.0\n",
      "train 13600 -64.50476838282808\n",
      "train 13700 67.34341431366938\n",
      "train 13800 100.0\n",
      "train 13900 71.31107328990078\n",
      "train 14000 100.0\n",
      "train 14100 100.0\n",
      "train 14200 -59.029426456839104\n",
      "train 14300 100.0\n",
      "train 14400 -70.73464205237727\n",
      "train 14500 -100.0\n",
      "train 14600 100.0\n",
      "train 14700 -100.0\n",
      "train 14800 83.57264711591782\n",
      "train 14900 100.0\n",
      "train 15000 100.0\n",
      "train 15100 -66.59938055282673\n",
      "train 15200 -100.0\n",
      "train 15300 -100.0\n",
      "train 15400 -100.0\n",
      "train 15500 -74.63725284897279\n",
      "train 15600 100.0\n",
      "train 15700 -84.72383869335331\n",
      "train 15800 100.0\n",
      "train 15900 -100.0\n",
      "train 16000 -100.0\n",
      "train 16100 84.65370939851371\n",
      "train 16200 100.0\n",
      "train 16300 73.36074066872033\n",
      "train 16400 100.0\n",
      "train 16500 100.0\n",
      "train 16600 -100.0\n",
      "train 16700 47.25901794687411\n",
      "train 16800 -100.0\n",
      "train 16900 -100.0\n",
      "train 17000 100.0\n",
      "train 17100 -100.0\n",
      "train 17200 47.2323112285016\n",
      "train 17300 100.0\n",
      "train 17400 77.1389235938276\n",
      "train 17500 100.0\n",
      "train 17600 -78.43634789892027\n",
      "train 17700 -100.0\n",
      "train 17800 -100.0\n",
      "train 17900 -100.0\n",
      "train 18000 100.0\n",
      "train 18100 100.0\n",
      "train 18200 -88.43542481913539\n",
      "train 18300 -79.28867338961022\n",
      "train 18400 -100.0\n",
      "train 18500 -100.0\n",
      "train 18600 100.0\n",
      "train 18700 -100.0\n",
      "train 18800 76.48311622729356\n",
      "train 18900 -100.0\n",
      "train 19000 100.0\n",
      "train 19100 -100.0\n",
      "train 19200 -84.85871126623437\n",
      "train 19300 -100.0\n",
      "train 19400 100.0\n",
      "train 19500 100.0\n",
      "train 19600 -69.74169923264526\n",
      "train 19700 81.68560030975816\n",
      "train 19800 -100.0\n",
      "train 19900 100.0\n",
      "train 20000 100.0\n",
      "train 20100 -100.0\n",
      "train 20200 100.0\n",
      "train 20300 -100.0\n",
      "train 20400 -100.0\n",
      "train 20500 100.0\n",
      "train 20600 100.0\n",
      "train 20700 100.0\n",
      "train 20800 -100.0\n",
      "train 20900 100.0\n",
      "train 21000 100.0\n",
      "train 21100 -100.0\n",
      "train 21200 -60.35995105013093\n",
      "train 21300 -100.0\n",
      "train 21400 -100.0\n",
      "train 21500 100.0\n",
      "train 21600 -100.0\n",
      "train 21700 -100.0\n",
      "train 21800 100.0\n",
      "train 21900 -100.0\n",
      "train 22000 100.0\n",
      "train 22100 -100.0\n",
      "train 22200 -100.0\n",
      "train 22300 -100.0\n",
      "train 22400 100.0\n",
      "train 22500 100.0\n",
      "train 22600 70.70182044602322\n",
      "train 22700 -100.0\n",
      "train 22800 -100.0\n",
      "train 22900 -100.0\n",
      "train 23000 100.0\n",
      "train 23100 -100.0\n",
      "train 23200 100.0\n",
      "train 23300 -86.0789184441746\n",
      "train 23400 -100.0\n",
      "train 23500 100.0\n",
      "train 23600 -100.0\n",
      "train 23700 -100.0\n",
      "train 23800 100.0\n",
      "train 23900 -69.79656223393285\n",
      "train 24000 100.0\n",
      "train 24100 100.0\n",
      "train 24200 100.0\n",
      "train 24300 100.0\n",
      "train 24400 -100.0\n",
      "train 24500 100.0\n",
      "train 24600 -100.0\n",
      "train 24700 100.0\n",
      "train 24800 100.0\n",
      "train 24900 -100.0\n",
      "train 25000 86.90306089472553\n",
      "train 25100 81.66866304231002\n",
      "train 25200 -100.0\n",
      "train 25300 100.0\n",
      "train 25400 -100.0\n",
      "train 25500 -100.0\n",
      "train 25600 100.0\n",
      "train 25700 -100.0\n",
      "train 25800 100.0\n",
      "train 25900 -100.0\n",
      "train 26000 -100.0\n",
      "train 26100 -100.0\n",
      "train 26200 100.0\n",
      "train 26300 -88.2269745107448\n",
      "train 26400 -84.00024416275087\n",
      "train 26500 -100.0\n",
      "train 26600 -83.33130650108693\n",
      "train 26700 100.0\n",
      "train 26800 -87.9649887508002\n",
      "train 26900 -100.0\n",
      "0: done!\n",
      "0.5806666666666667\n",
      "EPOCH: 1\n",
      "train 0 -100.0\n",
      "train 100 -61.23201762841364\n",
      "train 200 -100.0\n",
      "train 300 43.847103099140405\n",
      "train 400 100.0\n",
      "train 500 -67.19522859250479\n",
      "train 600 -100.0\n",
      "train 700 -100.0\n",
      "train 800 -100.0\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from lib.models import ResNet\n",
    "from lib.utils import train_parse_args\n",
    "from lib.utils.data import get_data_loaders\n",
    "\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "\n",
    "loaders = get_data_loaders(\n",
    "    imgs_dir=params['imgs_dir'],\n",
    "    labels_filename=params['labels_filename'],\n",
    "    batch_size=params['batch_size'],\n",
    "    n_imgs=params['n_imgs'],\n",
    "    label=params['label'])\n",
    "train_loader = loaders['train_loader']\n",
    "valid_loader = loaders['valid_loader']\n",
    "\n",
    "model = ResNet().to(device)\n",
    "criterion = BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "'''\n",
    "accuracy_scores_history = valid_phase(\n",
    "    model=model,\n",
    "    valid_loader=valid_loader,\n",
    "    device=device,\n",
    "    epoch=0)\n",
    "'''\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"EPOCH: {epoch}\")\n",
    "    loss_history = train_phase(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        epoch=epoch,\n",
    "        verbose=True)\n",
    "    accuracy_scores_history = valid_phase(\n",
    "        model=model,\n",
    "        valid_loader=valid_loader,\n",
    "        device=device,\n",
    "        epoch=epoch,\n",
    "        verbose=True)\n",
    "    avg_valid_accuracy = np.mean(np.array(accuracy_scores_history))\n",
    "    print(avg_valid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
